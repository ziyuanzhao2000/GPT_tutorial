{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a07fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp, random\n",
    "from typing import Any, Callable, Sequence\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d18fd0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/s5n2s3z54xb38h09mdwdc8sm0000gn/T/ipykernel_85187/2580067358.py:15: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  data = jnp.array(encode(text), dtype=jnp.int64)\n"
     ]
    }
   ],
   "source": [
    "### loading and processing dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "data = jnp.array(encode(text), dtype=jnp.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "095d943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c46c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split, key):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = random.randint(key, (batch_size,), 0, len(data) - block_size)\n",
    "    x = jnp.stack([data[i:i+block_size] for i in ix])\n",
    "    y = jnp.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "key = random.key(0)\n",
    "xb, yb = get_batch('train', key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89c1c596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "[[ 0 32 46 39 58  1 57 46]\n",
      " [57  6  1 40 63  1 63 53]\n",
      " [ 1 58 43 50 50  1 63 53]\n",
      " [ 0 37 53 59 56  1 51 53]]\n",
      "targets:\n",
      "(4, 8)\n",
      "[[32 46 39 58  1 57 46 43]\n",
      " [ 6  1 40 63  1 63 53 59]\n",
      " [58 43 50 50  1 63 53 59]\n",
      " [37 53 59 56  1 51 53 58]]\n",
      "----\n",
      "when input is [0] the target: 32\n",
      "when input is [0, 32] the target: 46\n",
      "when input is [0, 32, 46] the target: 39\n",
      "when input is [0, 32, 46, 39] the target: 58\n",
      "when input is [0, 32, 46, 39, 58] the target: 1\n",
      "when input is [0, 32, 46, 39, 58, 1] the target: 57\n",
      "when input is [0, 32, 46, 39, 58, 1, 57] the target: 46\n",
      "when input is [0, 32, 46, 39, 58, 1, 57, 46] the target: 43\n",
      "when input is [57] the target: 6\n",
      "when input is [57, 6] the target: 1\n",
      "when input is [57, 6, 1] the target: 40\n",
      "when input is [57, 6, 1, 40] the target: 63\n",
      "when input is [57, 6, 1, 40, 63] the target: 1\n",
      "when input is [57, 6, 1, 40, 63, 1] the target: 63\n",
      "when input is [57, 6, 1, 40, 63, 1, 63] the target: 53\n",
      "when input is [57, 6, 1, 40, 63, 1, 63, 53] the target: 59\n",
      "when input is [1] the target: 58\n",
      "when input is [1, 58] the target: 43\n",
      "when input is [1, 58, 43] the target: 50\n",
      "when input is [1, 58, 43, 50] the target: 50\n",
      "when input is [1, 58, 43, 50, 50] the target: 1\n",
      "when input is [1, 58, 43, 50, 50, 1] the target: 63\n",
      "when input is [1, 58, 43, 50, 50, 1, 63] the target: 53\n",
      "when input is [1, 58, 43, 50, 50, 1, 63, 53] the target: 59\n",
      "when input is [0] the target: 37\n",
      "when input is [0, 37] the target: 53\n",
      "when input is [0, 37, 53] the target: 59\n",
      "when input is [0, 37, 53, 59] the target: 56\n",
      "when input is [0, 37, 53, 59, 56] the target: 1\n",
      "when input is [0, 37, 53, 59, 56, 1] the target: 51\n",
      "when input is [0, 37, 53, 59, 56, 1, 51] the target: 53\n",
      "when input is [0, 37, 53, 59, 56, 1, 51, 53] the target: 58\n"
     ]
    }
   ],
   "source": [
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6db1c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.vmap, in_axes=(0, 0, None))\n",
    "def _multinomial(keys, probs, num_samples):\n",
    "    return jax.random.choice(keys, jnp.arange(len(probs)), \n",
    "                             shape=(num_samples,), \n",
    "                             p=probs)\n",
    "\n",
    "def multinomial(key, probs, num_samples):\n",
    "    batch_size = probs.shape[0]\n",
    "    keys = jax.random.split(key, batch_size)\n",
    "    return _multinomial(keys, probs, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23267b8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[2, 2, 1, 1, 2],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [1, 1, 2, 2, 2]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = random.key(0)\n",
    "multinomial(key, jnp.array([[0,1,1],\n",
    "                            [1,0,0],\n",
    "                            [0,1,1]]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "810dfa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 65) () 4.188377\n"
     ]
    }
   ],
   "source": [
    "# def cross_entropy(logits, targets, axis=-1):\n",
    "#     ### logits: [B, N, C], targets: [B, N]\n",
    "#     targets = jax.nn.one_hot(targets, logits.shape[-1])\n",
    "#     return -(logits * targets).sum()\n",
    "\n",
    "n_embd = 16\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(num_embeddings=vocab_size, \n",
    "                                              features=vocab_size)\n",
    "        \n",
    "    def __call__(self, idx):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, key, idx, max_new_tokens):\n",
    "        keys = random.split(key, max_new_tokens)\n",
    "        for i in range(max_new_tokens):\n",
    "            logits = self(idx)[:, -1, :]\n",
    "            probs = jax.nn.softmax(logits, axis=-1)\n",
    "            idx_next = multinomial(keys[i], probs, num_samples=1)\n",
    "            idx = jnp.concat((idx, idx_next), axis=1)\n",
    "        return idx\n",
    "\n",
    "def loss_fn(preds, targets):\n",
    "    B, T, C = preds.shape\n",
    "    logits = preds.reshape(B*T, C)\n",
    "    targets = targets.reshape(B*T)\n",
    "    targets = jax.nn.one_hot(targets, C)\n",
    "    loss = optax.losses.softmax_cross_entropy(logits, targets).mean()\n",
    "    return loss\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "x, y = get_batch('train', key)\n",
    "params = m.init(key, x)\n",
    "logits = m.apply(params, x)\n",
    "loss = loss_fn(logits, y)\n",
    "print(logits.shape, loss.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1649b226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.7436683], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing that cross entropy loss works correctly from optax\n",
    "logits = jnp.array([[0,0,0,1]], dtype=jnp.float32)\n",
    "labels = jnp.array([[0,0,0,1]], dtype=jnp.float32)\n",
    "optax.softmax_cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f41e531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!IkvgeEYJ EkhFBgkOqJXdbuEzswu!.b-z:AU3j:cxvmzaHGhTnK,aHeFfTWhdSvCo:;:ZrpFa-oJQwt:oC mT-si'xe;TV\n",
      "eFsI\n"
     ]
    }
   ],
   "source": [
    "context = jnp.zeros((1, 1), dtype=jnp.int32)\n",
    "autocomplete = m.apply(params, key=key, idx=context, max_new_tokens=100, method='generate')\n",
    "print(decode(autocomplete.tolist()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "04bc4e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  4.1692815\n",
      "Loss step 1000:  3.7298422\n",
      "Loss step 2000:  3.1380658\n",
      "Loss step 3000:  3.3327022\n",
      "Loss step 4000:  2.668402\n",
      "Loss step 5000:  2.6300232\n",
      "Loss step 6000:  3.137767\n",
      "Loss step 7000:  2.418674\n",
      "Loss step 8000:  2.8468523\n",
      "Loss step 9000:  2.821628\n",
      "CPU times: user 27.8 s, sys: 78.4 ms, total: 27.9 s\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rate = 1e-3\n",
    "\n",
    "tx = optax.adamw(learning_rate=learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def step(params, opt_state, xb, yb):\n",
    "    def loss(params, xb, yb):\n",
    "        preds = m.apply(params, xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        return loss\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss)(params, xb, yb)\n",
    "    updates, opt_state = tx.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "    \n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "    # make a new key for randomly getting a minibatch\n",
    "    data_key, key = random.split(key, 2)\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', data_key)\n",
    "\n",
    "    # evaluate the loss\n",
    "    params, opt_state, loss = step(params, opt_state, xb, yb)\n",
    "#     print(params)\n",
    "    if steps % 1000 == 0:\n",
    "        print('Loss step {}: '.format(steps), loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "492c8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bllee.\n",
      "Rof mes tyoorwedet, IUSoven ee me,\n",
      "\n",
      "NRThauntion FI! sie. d therdandst isuprerend, Yierurrin furtinghis bis r ppe thigre d pa':\n",
      "I:\n",
      "'d apo tout m m I:\n",
      "stir3had d owotlleashonctoofopllbemainotunthrssen:\n",
      "SIA:\n",
      "t grbalisee be thum he\n",
      "Angrou, s,lyrs KELAn ad Conde.\n",
      "-irr he tinced n:Fz;\n",
      "Morompr y rf ntith, doantheavey HYD:\n",
      "CHANLAxp'linene be, y\n",
      "llirt, bell y ICALoullowond, l implf whin\n",
      "\n",
      "WAMBer ton mu!GQShyssde; m ce Yo y,\n",
      "UCovere me'Sa\n",
      "\n",
      "'shaus, n,\n",
      "Twnof dreere I dn s tan:\n",
      "Qd.Enom hen wivithwP,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = jnp.zeros((1, 1), dtype=jnp.int32)\n",
    "autocomplete = m.apply(params, key=key, idx=context, max_new_tokens=500, method='generate')\n",
    "print(decode(autocomplete.tolist()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336668a",
   "metadata": {},
   "source": [
    "So far so good, and some notes on how jax+flax code is organized different from pytorch:\n",
    "1. the `__call__` method doesn't compute loss, and loss computation is moved out into `step` function that can be jitted for the training loop.\n",
    "2. some util functions are not available in jax, such as `multinomial` so one has to write custom versions of them.\n",
    "3. managing and updating optimizer state is more complicated than in pytorch, which just amounts to `optimizer.step`.\n",
    "\n",
    "The next step is to rewrite each of the classes in the pytorch implementation in jax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a03d1",
   "metadata": {},
   "source": [
    "```python\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * self.head_size**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) \n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4afa2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    batch_size = 32\n",
    "    block_size = 32 #k-gram\n",
    "    max_iters = 5000\n",
    "    eval_interval = 500\n",
    "    learning_rate = 5e-3\n",
    "    eval_iters = 200\n",
    "    n_embd = 32\n",
    "    n_head = 4\n",
    "    n_layer = 4\n",
    "    dropout = 0.2\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \n",
    "    config: Config\n",
    "    head_size: int\n",
    "        \n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.dropout = nn.Dropout(self.config.dropout)\n",
    "        \n",
    "    def __call__(self, x, deterministic=True):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = jnp.matmul(q, k.swapaxes(-2, -1)) * self.head_size**-0.5\n",
    "        wei = jnp.tril(wei)\n",
    "        wei = jnp.where(wei, wei, float('-inf'))\n",
    "        wei = jax.nn.softmax(wei, axis=-1)\n",
    "        wei = self.dropout(wei, deterministic=deterministic)\n",
    "        v = self.value(x)\n",
    "        out = jnp.matmul(wei, v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a22f92",
   "metadata": {},
   "source": [
    "Notes on some differences between jax+flax and pytorch:\n",
    "1. `flax.linen.Dense` doesn't need input shape,\n",
    "2. `jnp.tril` directly returns masked array so no need to store mask as a frozen variable\n",
    "3. Use `jnp.matmul` to explicitly to batched multiplication between matrices of shapes `(..., K, N)` and `(..., N, M)`.\n",
    "4. For `Dropout` that has different behavior at trainining vs inference this is controlled by passing the `deterministic` boolean variable rather than using `model.train()` or `model.eval()` (stateful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "135b8932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[  5., -inf, -inf, -inf, -inf],\n",
       "       [  9.,   8., -inf, -inf, -inf],\n",
       "       [  6.,   3.,   3., -inf, -inf],\n",
       "       [  4.,   3.,   3.,   6., -inf],\n",
       "       [  2.,   7.,   2.,   4.,   5.]], dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = random.randint(key, (5,5), 0, 10)\n",
    "arr = jnp.tril(arr)\n",
    "arr = jnp.where(arr, arr, float('-inf'))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6d07147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 8)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "head = Head(config=config, head_size=8)\n",
    "rand_x = random.normal(key, (config.batch_size, config.block_size, config.n_embd))\n",
    "params = head.init(rngs=key,\n",
    "                   x=rand_x)\n",
    "head.apply(params, rand_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f93ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniforge3-jax]",
   "language": "python",
   "name": "conda-env-miniforge3-jax-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
